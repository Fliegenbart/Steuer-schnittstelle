"""BelegSync â€“ Extraction Service mit LLM-nativem Source Grounding.

Extrahiert steuerlich relevante Daten aus OCR-Text via Ollama (Llama 3.1 8B).
Eigenes Source Grounding: Das LLM liefert fÃ¼r jeden Wert den exakten Quelltext-
Ausschnitt aus dem OCR-Text. So kann der Steuerberater jeden extrahierten Wert
direkt im Originaldokument nachvollziehen (Explainable AI).
"""
import os, json, re, logging
from typing import Optional
import httpx

logger = logging.getLogger(__name__)

OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.1:8b-instruct-q4_K_M")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ollama Extraction â€“ Prompt mit Source Grounding
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SYSTEM_PROMPT = """Du bist ein Experte fÃ¼r deutsche Steuerbelege. Du analysierst OCR-Text und extrahierst steuerlich relevante Daten als JSON.

REGELN:
- Antworte NUR mit einem JSON-Objekt, kein anderer Text
- Deutsche Zahlen (1.234,56) als Dezimalzahl im JSON-Wert: 1234.56
- Unbekannte Felder: null (nicht "null", nicht "", nicht 0)
- Datum im Format TT.MM.JJJJ
- Bei Handwerkerrechnungen/Nebenkostenabrechnungen: trenne Arbeitskosten (Â§35a absetzbar) von Materialkosten (nicht absetzbar)

QUELLENNACHWEISE (Source Grounding):
- FÃ¼r JEDEN extrahierten Wert gib zusÃ¤tzlich "quelle" an
- "quelle" = der EXAKTE Textausschnitt aus dem OCR-Text, aus dem du den Wert abgeleitet hast
- Kopiere den Text GENAU wie er im OCR-Text steht (inkl. Sonderzeichen, Leerzeichen)
- FÃ¼r abgeleitete Felder (beleg_typ, steuer_kategorie, skr03_konto): quelle = null
- Format pro Feld: {"wert": <extrahierter_wert>, "quelle": "<exakter_OCR_text>" oder null}

FELDER:
- beleg_typ: rechnung | handwerkerrechnung | lohnsteuerbescheinigung | spendenbescheinigung | versicherungsnachweis | kontoauszug | nebenkostenabrechnung | arztrechnung | fahrtkosten | bewirtungsbeleg | sonstig
- aussteller: Name der Firma/Person
- beschreibung: Kurzbeschreibung (max 100 Zeichen)
- betrag_brutto: Gesamtbetrag inkl. MwSt
- betrag_netto: Nettobetrag ohne MwSt
- mwst_satz: MwSt-Prozentsatz (7 oder 19)
- mwst_betrag: MwSt-Betrag in Euro
- datum_beleg: Rechnungs-/Belegdatum
- rechnungsnummer: Rechnungs- oder Belegnummer
- steuer_kategorie: Werbungskosten | Sonderausgaben | AuÃŸergewÃ¶hnliche Belastungen | Haushaltsnahe Dienstleistungen Â§35a | Handwerkerleistungen Â§35a | Vorsorgeaufwendungen | Spenden und MitgliedsbeitrÃ¤ge | EinkÃ¼nfte nichtselbstÃ¤ndige Arbeit | EinkÃ¼nfte selbstÃ¤ndige Arbeit | EinkÃ¼nfte Vermietung/Verpachtung
- skr03_konto: 4-stelliges SKR03-Konto
- arbeitskosten_35a: Arbeits-/Lohnanteil (Â§35a absetzbar)
- materialkosten: Materialanteil (NICHT absetzbar)"""

ONE_SHOT_EXAMPLE = """
BEISPIEL:
OCR-Text: "Rechnung Nr. 2024-0815\\nMalermeister Schmidt GmbH\\nHauptstr. 12, 20095 Hamburg\\nAnstricharbeiten Wohnzimmer\\nArbeitskosten: 1.200,00 â‚¬\\nMaterial: 340,00 â‚¬\\nNetto: 1.540,00 â‚¬\\nMwSt 19%: 292,60 â‚¬\\nBrutto: 1.832,60 â‚¬\\nDatum: 15.03.2024"

Antwort:
{"beleg_typ": {"wert": "handwerkerrechnung", "quelle": null}, "aussteller": {"wert": "Malermeister Schmidt GmbH", "quelle": "Malermeister Schmidt GmbH"}, "beschreibung": {"wert": "Anstricharbeiten Wohnzimmer", "quelle": "Anstricharbeiten Wohnzimmer"}, "betrag_brutto": {"wert": 1832.60, "quelle": "Brutto: 1.832,60 \\u20ac"}, "betrag_netto": {"wert": 1540.00, "quelle": "Netto: 1.540,00 \\u20ac"}, "mwst_satz": {"wert": 19, "quelle": "MwSt 19%"}, "mwst_betrag": {"wert": 292.60, "quelle": "MwSt 19%: 292,60 \\u20ac"}, "datum_beleg": {"wert": "15.03.2024", "quelle": "Datum: 15.03.2024"}, "rechnungsnummer": {"wert": "2024-0815", "quelle": "Rechnung Nr. 2024-0815"}, "steuer_kategorie": {"wert": "Handwerkerleistungen \\u00a735a", "quelle": null}, "skr03_konto": {"wert": "4946", "quelle": null}, "arbeitskosten_35a": {"wert": 1200.00, "quelle": "Arbeitskosten: 1.200,00 \\u20ac"}, "materialkosten": {"wert": 340.00, "quelle": "Material: 340,00 \\u20ac"}}
"""

USER_PROMPT = """Analysiere diesen OCR-Text und extrahiere die steuerlich relevanten Daten als JSON mit Quellennachweisen:

{text}"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Response Parsing & Cleaning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _parse_json_from_llm(raw: str) -> Optional[dict]:
    """Robust JSON-Parsing aus LLM-Output (mit/ohne Markdown-Fences)."""
    if not raw or not raw.strip():
        return None

    # Versuch 1: Markdown ```json ... ``` Block
    fence_match = re.search(r'```(?:json)?\s*(\{[\s\S]*?\})\s*```', raw)
    if fence_match:
        try:
            return json.loads(fence_match.group(1))
        except json.JSONDecodeError:
            pass

    # Versuch 2: Erstes JSON-Objekt im Text
    json_match = re.search(r'\{[\s\S]*\}', raw)
    if json_match:
        try:
            return json.loads(json_match.group())
        except json.JSONDecodeError:
            pass

    # Versuch 3: Ganzer Text als JSON
    try:
        return json.loads(raw.strip())
    except json.JSONDecodeError:
        pass

    return None


def _unwrap_sourced_response(data: dict) -> tuple:
    """Entpacke {wert, quelle}-Paare in flache Werte + Quellen-Dict.

    Tolerant: Akzeptiert gemischt flat/nested Antworten.

    Returns:
        (flat_data, source_quotes) â€“ z.B.:
        flat_data:      {"aussteller": "Schmidt GmbH", "betrag_brutto": 1832.60}
        source_quotes:  {"aussteller": "Malermeister Schmidt GmbH", "betrag_brutto": "Brutto: 1.832,60 â‚¬"}
    """
    flat = {}
    quotes = {}
    for key, value in data.items():
        if isinstance(value, dict) and "wert" in value:
            # Nested {wert, quelle} Format
            flat[key] = value["wert"]
            if value.get("quelle"):
                quotes[key] = str(value["quelle"])
        else:
            # Flat value (LLM hat quelle-Anweisung ignoriert)
            flat[key] = value
    return flat, quotes


def _clean_extracted_data(data: dict) -> dict:
    """Bereinige LLM-Output: String-Nulls, leere Strings, ungÃ¼ltige Werte."""
    cleaned = {}
    for key, value in data.items():
        # "null", "None", "", "N/A" â†’ None
        if isinstance(value, str) and value.strip().lower() in ("null", "none", "", "n/a", "nicht angegeben", "unbekannt"):
            cleaned[key] = None
        # 0 bei optionalen GeldbetrÃ¤gen â†’ None (LLM gibt oft 0.00 statt null)
        elif isinstance(value, (int, float)) and value == 0 and key in (
            "betrag_netto", "mwst_betrag", "mwst_satz", "arbeitskosten_35a", "materialkosten"
        ):
            cleaned[key] = None
        else:
            cleaned[key] = value
    return cleaned


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  LLM-natives Source Grounding (Quellen-Matching)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _build_source_spans_from_quotes(ocr_text: str, quotes: dict) -> list:
    """Lokalisiert LLM-Quellennachweise im OCR-Text.

    Das LLM hat fÃ¼r jeden Wert ein 'quelle'-Feld geliefert (exakter Textausschnitt).
    Hier finden wir die Position dieses Ausschnitts im Original-OCR-Text.
    """
    spans = []
    for feld, quote in quotes.items():
        if not quote or not isinstance(quote, str) or len(quote.strip()) < 2:
            continue

        span = _locate_quote_in_text(ocr_text, quote.strip(), feld)
        if span:
            spans.append(span)
        else:
            logger.debug(f"Source quote not found for '{feld}': '{quote[:50]}'")

    return spans


def _locate_quote_in_text(ocr_text: str, quote: str, feld: str) -> Optional[dict]:
    """4-stufige Matching-Kaskade: Findet ein LLM-Zitat im OCR-Text.

    Tier 1: Exakte Suche
    Tier 2: Case-insensitive
    Tier 3: Normalisiert (Whitespace kollabiert)
    Tier 4: Fuzzy Sliding Window (Bigram-Dice â‰¥ 0.80)
    """
    # Tier 1: Exakt
    idx = ocr_text.find(quote)
    if idx >= 0:
        return {"start": idx, "end": idx + len(quote), "text": quote, "feld": feld}

    # Tier 2: Case-insensitive
    idx = ocr_text.lower().find(quote.lower())
    if idx >= 0:
        matched = ocr_text[idx:idx + len(quote)]
        return {"start": idx, "end": idx + len(quote), "text": matched, "feld": feld}

    # Tier 3: Normalisiert (Whitespace, ZeilenumbrÃ¼che)
    norm_quote = _normalize_text(quote)
    norm_ocr = _normalize_text(ocr_text)
    norm_idx = norm_ocr.find(norm_quote)
    if norm_idx >= 0:
        orig_start, orig_end = _map_normalized_pos(ocr_text, norm_ocr, norm_idx, len(norm_quote))
        matched = ocr_text[orig_start:orig_end]
        return {"start": orig_start, "end": orig_end, "text": matched, "feld": feld}

    # Tier 4: Fuzzy (nur fÃ¼r nicht-triviale Strings)
    if len(quote) >= 5:
        result = _fuzzy_slide_match(ocr_text, quote, threshold=0.80)
        if result:
            start, end = result
            return {"start": start, "end": end, "text": ocr_text[start:end], "feld": feld}

    return None


def _normalize_text(text: str) -> str:
    """Normalisiert Text: Whitespace kollabieren, Lowercase."""
    return re.sub(r'\s+', ' ', text).strip().lower()


def _map_normalized_pos(original: str, normalized: str, norm_start: int, norm_len: int) -> tuple:
    """Mappt Position im normalisierten Text zurÃ¼ck auf Original-Positionen.

    BerÃ¼cksichtigt kollabierte Whitespace-Sequenzen.
    """
    orig_pos = 0
    norm_pos = 0
    orig_start = None

    while norm_pos < len(normalized) and orig_pos < len(original):
        if norm_pos == norm_start and orig_start is None:
            orig_start = orig_pos
        if norm_pos == norm_start + norm_len:
            return (orig_start or 0, orig_pos)

        if normalized[norm_pos] == ' ' and original[orig_pos] in (' ', '\t', '\n', '\r'):
            norm_pos += 1
            while orig_pos < len(original) and original[orig_pos] in (' ', '\t', '\n', '\r'):
                orig_pos += 1
        else:
            norm_pos += 1
            orig_pos += 1

    if orig_start is not None:
        return (orig_start, orig_pos)
    return (0, min(norm_len, len(original)))


def _fuzzy_slide_match(ocr_text: str, quote: str, threshold: float = 0.80) -> Optional[tuple]:
    """Sliding-Window Fuzzy-Match mit Bigram-Dice-Koeffizient.

    Schiebt ein Fenster (Â±20% der Quote-LÃ¤nge) Ã¼ber den OCR-Text und
    findet die Position mit der hÃ¶chsten Ã„hnlichkeit.
    """
    qlen = len(quote)
    if qlen < 5 or len(ocr_text) < qlen:
        return None

    quote_lower = quote.lower()
    text_lower = ocr_text.lower()
    quote_bigrams = _bigrams(quote_lower)

    if not quote_bigrams:
        return None

    best_ratio = 0.0
    best_pos = None

    # FenstergrÃ¶ÃŸen: exakt, Â±10%, Â±20%
    window_sizes = sorted(set([
        qlen,
        max(3, int(qlen * 0.9)),
        min(len(ocr_text), int(qlen * 1.1)),
        max(3, int(qlen * 0.8)),
        min(len(ocr_text), int(qlen * 1.2)),
    ]))

    for window_size in window_sizes:
        if window_size > len(ocr_text):
            continue
        for i in range(0, len(ocr_text) - window_size + 1, 1):
            candidate = text_lower[i:i + window_size]
            cand_bigrams = _bigrams(candidate)
            if not cand_bigrams:
                continue

            # Dice-Koeffizient
            overlap = len(quote_bigrams & cand_bigrams)
            ratio = 2.0 * overlap / (len(quote_bigrams) + len(cand_bigrams))

            if ratio > best_ratio:
                best_ratio = ratio
                best_pos = (i, i + window_size)

    if best_ratio >= threshold and best_pos:
        return best_pos
    return None


def _bigrams(s: str) -> set:
    """Erzeugt Bigram-Set aus einem String."""
    return set(s[i:i+2] for i in range(len(s) - 1)) if len(s) >= 2 else set()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Post-hoc Source Grounding (Fallback)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _build_source_spans(ocr_text: str, attrs: dict) -> list:
    """Fallback: Findet extrahierte Werte im OCR-Text per Textsuche.
    Wird genutzt wenn das LLM keine quelle-Felder liefert."""
    spans = []
    text_lower = ocr_text.lower()

    search_fields = {
        "aussteller": attrs.get("aussteller"),
        "betrag_brutto": attrs.get("betrag_brutto"),
        "betrag_netto": attrs.get("betrag_netto"),
        "mwst_betrag": attrs.get("mwst_betrag"),
        "datum_beleg": attrs.get("datum_beleg"),
        "rechnungsnummer": attrs.get("rechnungsnummer"),
        "arbeitskosten_35a": attrs.get("arbeitskosten_35a"),
        "materialkosten": attrs.get("materialkosten"),
        "beschreibung": attrs.get("beschreibung"),
    }

    for feld, value in search_fields.items():
        if value is None:
            continue
        value_str = str(value)
        if not value_str or value_str == "null":
            continue

        # Exakte Suche
        idx = ocr_text.find(value_str)
        if idx >= 0:
            spans.append({"start": idx, "end": idx + len(value_str), "text": value_str, "feld": feld})
            continue

        # Deutsche Zahlenformate (1234.56 â†’ 1.234,56)
        if re.match(r'^\d+\.?\d*$', value_str):
            german = _to_german_number(value_str)
            for variant in [german, value_str.replace(".", ",")]:
                idx = ocr_text.find(variant)
                if idx >= 0:
                    spans.append({"start": idx, "end": idx + len(variant), "text": variant, "feld": feld})
                    break
            else:
                # Ohne Tausender-Trenner (z.B. "1200,00")
                simple = value_str.split(".")[0] + "," + (value_str.split(".")[1] if "." in value_str else "00")
                idx = ocr_text.find(simple)
                if idx >= 0:
                    spans.append({"start": idx, "end": idx + len(simple), "text": simple, "feld": feld})

        # Case-insensitive fÃ¼r Textfelder
        if feld in ("aussteller", "rechnungsnummer", "beschreibung"):
            idx = text_lower.find(value_str.lower())
            if idx >= 0:
                spans.append({"start": idx, "end": idx + len(value_str), "text": ocr_text[idx:idx+len(value_str)], "feld": feld})
            elif feld == "aussteller" and len(value_str) > 5:
                words = value_str.split()[:2]
                if len(words) >= 2:
                    pattern = re.escape(words[0]) + r'[\s\-]+' + re.escape(words[1])
                    match = re.search(pattern, ocr_text, re.IGNORECASE)
                    if match:
                        spans.append({"start": match.start(), "end": match.end(), "text": match.group(), "feld": feld})

    return spans


def _to_german_number(n: str) -> str:
    """Konvertiert 1234.56 â†’ 1.234,56."""
    try:
        f = float(n)
        integer_part = int(f)
        decimal_part = round(f - integer_part, 2)
        formatted_int = f"{integer_part:,}".replace(",", ".")
        if decimal_part > 0:
            dec_str = f"{decimal_part:.2f}"[2:]
            return f"{formatted_int},{dec_str}"
        return f"{formatted_int},00"
    except (ValueError, TypeError):
        return n


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Confidence Assessment
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _assess_confidence(attrs: dict, spans: list = None) -> str:
    """Bewertet ExtraktionsqualitÃ¤t: Feld-VollstÃ¤ndigkeit + Source Grounding."""
    required = ["beleg_typ", "betrag_brutto", "aussteller", "datum_beleg"]
    found = sum(1 for f in required if attrs.get(f) is not None)

    # Bonus: Wie viele Felder sind source-gegrundet?
    grounded = set(s["feld"] for s in (spans or []))
    grounded_count = sum(1 for f in ["betrag_brutto", "aussteller", "datum_beleg"] if f in grounded)

    if found >= 4 and grounded_count >= 2:
        return "hoch"
    elif found >= 3:
        return "mittel"
    elif found >= 2:
        return "mittel"
    return "niedrig"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ollama Extraction
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def extract_with_ollama(ocr_text: str, retry: bool = True) -> dict:
    """Extrahiert Daten via Ollama mit LLM-nativem Source Grounding."""
    prompt = f"{SYSTEM_PROMPT}\n{ONE_SHOT_EXAMPLE}\n{USER_PROMPT.format(text=ocr_text[:4000])}"

    try:
        async with httpx.AsyncClient(timeout=180.0) as client:
            resp = await client.post(f"{OLLAMA_URL}/api/generate", json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": 0.1, "num_predict": 2000}
            })
            resp.raise_for_status()
            raw = resp.json().get("response", "")
            logger.info(f"Ollama raw response length: {len(raw)}")

            data = _parse_json_from_llm(raw)

            # Retry einmal bei Parse-Fehler
            if data is None and retry:
                logger.warning("JSON parse failed, retrying extraction...")
                return await extract_with_ollama(ocr_text, retry=False)

            if data is None:
                logger.error(f"Could not parse JSON from Ollama response: {raw[:200]}")
                return {
                    "extrahierte_daten": {},
                    "quellreferenzen": [],
                    "methode": "ollama_direkt",
                    "konfidenz": "niedrig"
                }

            # Entpacke {wert, quelle}-Paare (tolerant bei flat responses)
            flat_data, source_quotes = _unwrap_sourced_response(data)
            flat_data = _clean_extracted_data(flat_data)

            # PrimÃ¤r: LLM-native Source Spans aus quelle-Feldern
            spans = _build_source_spans_from_quotes(ocr_text, source_quotes)
            grounded_fields = {s["feld"] for s in spans}

            logger.info(f"LLM source grounding: {len(spans)} spans from {len(source_quotes)} quotes")

            # Fallback: Post-hoc Matching fÃ¼r un-gegrundete Felder
            fallback_data = {k: v for k, v in flat_data.items() if k not in grounded_fields}
            if fallback_data:
                fallback_spans = _build_source_spans(ocr_text, fallback_data)
                spans.extend(fallback_spans)
                logger.info(f"Fallback grounding: {len(fallback_spans)} additional spans")

            return {
                "extrahierte_daten": flat_data,
                "quellreferenzen": spans,
                "methode": "ollama_direkt",
                "konfidenz": _assess_confidence(flat_data, spans)
            }

    except httpx.ConnectError:
        logger.error(f"Ollama not reachable at {OLLAMA_URL}")
    except httpx.TimeoutException:
        logger.error(f"Ollama timeout after 180s")
    except Exception as e:
        logger.error(f"Ollama error: {e}")

    return {
        "extrahierte_daten": {},
        "quellreferenzen": [],
        "methode": "fehler",
        "konfidenz": "niedrig"
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Auto-Kontierung (SKR03 Mapping)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

KONTIERUNG_MAP = {
    "handwerkerrechnung": ("4946", "Fremdleistungen", "3"),
    "rechnung": ("4900", "Sonst. betriebl. Aufwend.", "3"),
    "spendenbescheinigung": ("6300", "Sonst. betriebl. Aufwend.", ""),
    "bewirtungsbeleg": ("4650", "Bewirtungskosten", "3"),
    "fahrtkosten": ("4500", "Fahrzeugkosten", ""),
    "arztrechnung": ("4900", "Sonst. betriebl. Aufwend.", ""),
    "versicherungsnachweis": ("4300", "Versicherungen", ""),
    "nebenkostenabrechnung": ("4210", "Miete", ""),
    "lohnsteuerbescheinigung": ("4120", "GehÃ¤lter", ""),
}


def auto_kontierung(beleg_typ: str, mwst_satz: float = None) -> dict:
    """Auto-Kontierung: SKR03-Konto anhand Belegtyp zuweisen."""
    entry = KONTIERUNG_MAP.get(beleg_typ, ("4900", "Sonst. betriebl. Aufwend.", ""))
    bu = entry[2]
    if mwst_satz and not bu:
        bu = "3" if mwst_satz >= 15 else ("2" if mwst_satz >= 5 else "")
    return {"skr03_konto": entry[0], "skr03_bezeichnung": entry[1], "bu_schluessel": bu}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  BBox Enrichment (Geometric Source Grounding)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _enrich_spans_with_bboxes(spans: list, ocr_data: dict) -> list:
    """Enrich text-based source spans with geometric bounding boxes.

    For each span (identified by char_start/end), find all OCR words whose
    character range overlaps and compute the union bounding box.

    This enables the frontend to highlight the exact region on the
    original PDF/image where the extracted value appears.
    """
    if not ocr_data or "pages" not in ocr_data:
        return spans

    # Build flat list of all words across pages (already have global char offsets)
    all_words = []
    for page in ocr_data["pages"]:
        page_num = page.get("page", 1)
        for word in page.get("words", []):
            if "char_start" in word and "char_end" in word:
                all_words.append({**word, "page": page_num})

    if not all_words:
        return spans

    enriched = []
    for span in spans:
        span_start = span.get("start")
        span_end = span.get("end")

        if span_start is None or span_end is None:
            enriched.append(span)
            continue

        # Find all words overlapping with this span's character range
        matching_words = []
        for w in all_words:
            w_start = w["char_start"]
            w_end = w["char_end"]
            # Overlap check: ranges overlap if start < other_end AND end > other_start
            if w_start < span_end and w_end > span_start:
                matching_words.append(w)

        if matching_words:
            # Compute union bounding box
            min_x = min(w["x"] for w in matching_words)
            min_y = min(w["y"] for w in matching_words)
            max_right = max(w["x"] + w["w"] for w in matching_words)
            max_bottom = max(w["y"] + w["h"] for w in matching_words)
            page = matching_words[0]["page"]

            span_with_bbox = {
                **span,
                "bbox": {
                    "x": min_x,
                    "y": min_y,
                    "w": max_right - min_x,
                    "h": max_bottom - min_y,
                    "page": page,
                }
            }
            enriched.append(span_with_bbox)
            logger.debug(f"BBox for '{span.get('feld')}': page {page}, ({min_x},{min_y}) {max_right - min_x}x{max_bottom - min_y}")
        else:
            # No matching words â†’ keep span without bbox (frontend falls back to text view)
            enriched.append(span)

    bbox_count = sum(1 for s in enriched if "bbox" in s)
    logger.info(f"BBox enrichment: {bbox_count}/{len(enriched)} spans got bounding boxes")
    return enriched


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Main Entry Point
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def extract_beleg(ocr_text: str, ocr_data: dict = None) -> dict:
    """Hauptfunktion: Extrahiert Belegdaten via Ollama + Auto-Kontierung.

    Args:
        ocr_text: Full OCR text
        ocr_data: Optional word-level geometry from OCR service
                  {pages: [{page, width, height, words: [{x,y,w,h,text,conf,char_start,char_end}]}]}
    """
    result = await extract_with_ollama(ocr_text)

    # Auto-Kontierung falls nicht vom LLM geliefert
    data = result.get("extrahierte_daten", {})
    if data.get("beleg_typ") and not data.get("skr03_konto"):
        mwst = None
        try:
            mwst = float(data.get("mwst_satz", 0))
        except (ValueError, TypeError):
            pass
        kont = auto_kontierung(data["beleg_typ"], mwst)
        data.update(kont)
        result["extrahierte_daten"] = data

    # Enrich source spans with bounding boxes from OCR geometry
    spans = result.get("quellreferenzen", [])
    if ocr_data and spans:
        result["quellreferenzen"] = _enrich_spans_with_bboxes(spans, ocr_data)

    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Missing Documents Detection
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ERWARTETE_BELEGE = {
    "Pflicht": [
        ("lohnsteuerbescheinigung", "Lohnsteuerbescheinigung"),
    ],
    "HÃ¤ufig relevant": [
        ("versicherungsnachweis", "Krankenversicherung"),
        ("spendenbescheinigung", "Spendenbescheinigungen"),
        ("handwerkerrechnung", "Handwerkerrechnungen (Â§35a)"),
        ("nebenkostenabrechnung", "Nebenkostenabrechnung"),
    ],
    "PrÃ¼fen": [
        ("arztrechnung", "Arztrechnungen (auÃŸergew. Belastungen)"),
        ("fahrtkosten", "Fahrtkosten (Pendlerpauschale)"),
        ("bewirtungsbeleg", "Bewirtungsbelege"),
    ],
}

def detect_missing(beleg_typen: list[str]) -> dict:
    vorhandene = set(beleg_typen)
    fehlend, empfehlungen = [], []
    for prio, items in ERWARTETE_BELEGE.items():
        for typ, label in items:
            if typ not in vorhandene:
                fehlend.append(label)
                icon = "ğŸ”´" if prio == "Pflicht" else ("ğŸŸ¡" if prio == "HÃ¤ufig relevant" else "ğŸ”µ")
                empfehlungen.append(f"{icon} {prio}: {label}")
    return {"fehlende": fehlend, "vorhandene": list(vorhandene), "empfehlungen": empfehlungen}
